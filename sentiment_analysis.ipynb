{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segmentWords(s): \n",
    "    return s.split()\n",
    "\n",
    "def readFile(fileName):\n",
    "    # Function for reading file\n",
    "    # input: filename as string\n",
    "    # output: contents of file as list containing single words\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = segmentWords('\\n'.join(contents))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dataframe containing the counts of each word in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "for c in os.listdir(\"data_training/train\"):\n",
    "    directory = \"data_training/train/\" + c\n",
    "    for f in os.listdir(directory):\n",
    "        words = readFile(directory + \"/\" + f)\n",
    "        e = {x:words.count(x) for x in words}\n",
    "        #e['__FileID__'] = directory\n",
    "        e['__CLASS__'] = directory[-3:len(directory)]\n",
    "        \n",
    "        d.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from d - make sure to fill all the nan values with zeros.\n",
    "\n",
    "*Hint: Consider the `fillna()` function for Dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['\u0005', '\u0013earth', '\u0013goodies', '\u0013if', '\u0013ripley', '\u0013suspend', '\u0013they',\n",
       "       '\u0013white\u0014', '\u0014', '\u0016',\n",
       "       ...\n",
       "       'zukovsky', 'zundel', 'zurg's', 'zweibel', 'zwick', 'zwick's',\n",
       "       'zwigoff's', 'zycie', 'zycie'', '|'],\n",
       "      dtype='object', length=42775)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(0)\n",
    "data.shape\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = list(range(1129)) + list(range(1130, 1400))\n",
    "len(indices)\n",
    "\n",
    "X = data.iloc[:, indices].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1399)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "totalcount = 0\n",
    "for i in data['__CLASS__']:\n",
    "    if i == 'pos':\n",
    "        count+=1\n",
    "    totalcount+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = []\n",
    "for x in data['__CLASS__'].values:\n",
    "    if x == \"pos\":\n",
    "        r+=[1.0]\n",
    "    else:\n",
    "        r+=[0.0]\n",
    "r = np.asarray(r)\n",
    "#####NOTE: 1.0 is pos, 0.0 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1129"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.get_loc('__CLASS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Sample 80% of your dataframe to be the training data\n",
    "\n",
    "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, r, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words\n",
    "\n",
    "*Hint: Try looking at the Dataframe `drop()` function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "X_test.shape\n",
    "y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Logistic Regression\n",
    "* Use sklearn's linear_model.LogisticRegression() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logger = LogisticRegression()\n",
    "logger.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81785714285714284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61428571428571432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 1399)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lor = LogisticRegression(C = 0.1)\n",
    "#only way is to decrease regularization term which is counter productive\n",
    "lor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70714285714285718"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lor.score(X_train, y_train)\n",
    "#no significant difference, so not really much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.59285714285714286,\n",
       " 0.59999999999999998,\n",
       " 0.6071428571428571,\n",
       " 0.62142857142857144,\n",
       " 0.61428571428571432,\n",
       " 0.5892857142857143,\n",
       " 0.5892857142857143]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_z = []\n",
    "plot_c = []\n",
    "plot_d = []\n",
    "\n",
    "C = [.0001, .001, .01, .1, 1, 10, 100]\n",
    "for c in C:\n",
    "    plot_d.append(i)\n",
    "    plot_c.append(c)\n",
    "    q = LogisticRegression(C = c, max_iter = 100)\n",
    "    cf = q.fit(X_train, y_train)\n",
    "    plot_z.append(cf.score(X_test, y_test))\n",
    "index = [i for i in range(len(plot_z)) if plot_z[i] == max(plot_z)][0]\n",
    "plot_z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62142857142857144"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "* In the backward stepsize selection method, you can remove coefficients and the corresponding x columns, where the coefficient is more than a particular amount away from the mean - you can choose how far from the mean is reasonable.\n",
    "\n",
    "*Hint: Numpy's `argwhere()` might be useful here*  \n",
    "*Hint: Instead of defining a hard-coded constant to determine which features to keep or remove, consider using values relative to the distribution of the weight magnitudes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODODODODODOD\n",
    "means = []\n",
    "for x in data.columns:\n",
    "    if x == \"__CLASS__\":\n",
    "        continue\n",
    "    means.append(data[x].mean())\n",
    "means = np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zcores = [(x - means.mean())/means.std() for x in means ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.02320851794273061,\n",
       " 2.2448671439835119,\n",
       " 23.702476259874629,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.030712490356051517,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.032588483459381735,\n",
       " -0.026960504149391065,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.030712490356051517,\n",
       " -0.028836497252721284,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.03821646276937242,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.040092455872702638,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.22067058549019869,\n",
       " -0.041968448976032864,\n",
       " 0.063087164810459756,\n",
       " 0.19065469583691511,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.02320851794273061,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.025084511046060836,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.015704545529409707,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.034464476562711968,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.028836497252721284,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.070591137223780659,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.006324580012758583,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " 15.091667915588896,\n",
       " 15.275515239715258,\n",
       " 1.2149469302552183,\n",
       " -0.025084511046060836,\n",
       " -0.028836497252721284,\n",
       " -0.040092455872702638,\n",
       " 0.0086833648138832185,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " 101.49803426187574,\n",
       " 1.6745652405711235,\n",
       " 1.4907179164447613,\n",
       " -0.030712490356051517,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.0086833648138832185,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.034464476562711968,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 86.398165773170774,\n",
       " -0.011952559322749259,\n",
       " -0.040092455872702638,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.027443295847185481,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " 0.10435901308372471,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " 0.22254657859352894,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.019456531736070159,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.12687093032368743,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.030712490356051517,\n",
       " -0.04384444207936309,\n",
       " 0.010559357917213449,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.034464476562711968,\n",
       " -0.040092455872702638,\n",
       " -0.021332524839400385,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.013828552426079483,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " 0.010559357917213449,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.0030553855038925478,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.0030553855038925478,\n",
       " -0.021332524839400385,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " 0.04245124067382728,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.019456531736070159,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.006324580012758583,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.013828552426079483,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.028836497252721284,\n",
       " -0.04384444207936309,\n",
       " -0.006324580012758583,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.034464476562711968,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.034464476562711968,\n",
       " -0.040092455872702638,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.028836497252721284,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.040092455872702638,\n",
       " -0.041968448976032864,\n",
       " -0.030712490356051517,\n",
       " -0.021332524839400385,\n",
       " -0.011952559322749259,\n",
       " -0.040092455872702638,\n",
       " -0.026960504149391065,\n",
       " -0.03821646276937242,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.021332524839400385,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.034464476562711968,\n",
       " -0.0025725938060981324,\n",
       " -0.028836497252721284,\n",
       " -0.041968448976032864,\n",
       " -0.030712490356051517,\n",
       " -0.021332524839400385,\n",
       " -0.034464476562711968,\n",
       " -0.030712490356051517,\n",
       " -0.04384444207936309,\n",
       " -0.034464476562711968,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.04384444207936309,\n",
       " -0.025084511046060836,\n",
       " -0.04384444207936309,\n",
       " -0.028836497252721284,\n",
       " -0.04384444207936309,\n",
       " -0.032588483459381735,\n",
       " -0.028836497252721284,\n",
       " -0.0082005731160888079,\n",
       " -0.011952559322749259,\n",
       " -0.040092455872702638,\n",
       " -0.040092455872702638,\n",
       " -0.034464476562711968,\n",
       " -0.04384444207936309,\n",
       " -0.0082005731160888079,\n",
       " -0.040092455872702638,\n",
       " -0.019456531736070159,\n",
       " -0.040092455872702638,\n",
       " -0.021332524839400385,\n",
       " -0.04384444207936309,\n",
       " -0.028836497252721284,\n",
       " -0.04384444207936309,\n",
       " -0.015704545529409707,\n",
       " -0.04384444207936309,\n",
       " -0.028836497252721284,\n",
       " -0.034464476562711968,\n",
       " -0.028836497252721284,\n",
       " -0.03821646276937242,\n",
       " -0.017580538632739933,\n",
       " -0.02320851794273061,\n",
       " -0.034464476562711968,\n",
       " -0.015704545529409707,\n",
       " -0.03821646276937242,\n",
       " 0.0011793924005623182,\n",
       " -0.028836497252721284,\n",
       " -0.004448586909428353,\n",
       " -0.032588483459381735,\n",
       " 0.0218153165371948,\n",
       " -0.030712490356051517,\n",
       " -0.00069660070276790259,\n",
       " -0.019456531736070159,\n",
       " 0.064963157913789982,\n",
       " -0.006324580012758583,\n",
       " -0.04384444207936309,\n",
       " 0.078095109637101576,\n",
       " -0.028836497252721284,\n",
       " -0.04384444207936309,\n",
       " 0.061211171707129544,\n",
       " -0.02320851794273061,\n",
       " -0.017580538632739933,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " 0.45892170961313733,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " 0.046203226880487745,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.017580538632739933,\n",
       " -0.04384444207936309,\n",
       " 0.01243535102054367,\n",
       " -0.04384444207936309,\n",
       " -0.034464476562711968,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.027443295847185481,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.004448586909428353,\n",
       " -0.041968448976032864,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.041968448976032864,\n",
       " -0.025084511046060836,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.028836497252721284,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.0082005731160888079,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.19440668204357556,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " 0.025567302743855252,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.040092455872702638,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.032588483459381735,\n",
       " -0.025084511046060836,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.02320851794273061,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " 0.061211171707129544,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.0049313786072227687,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.030712490356051517,\n",
       " -0.040092455872702638,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " 0.051831206190478415,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.014311344123873899,\n",
       " -0.0025725938060981324,\n",
       " -0.04384444207936309,\n",
       " -0.036340469666042194,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.034464476562711968,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.021332524839400385,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.040092455872702638,\n",
       " 0.04245124067382728,\n",
       " -0.040092455872702638,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.03821646276937242,\n",
       " 0.01243535102054367,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.021332524839400385,\n",
       " -0.041968448976032864,\n",
       " -0.017580538632739933,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.026960504149391065,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.032588483459381735,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " 0.014311344123873899,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " 0.02369130964052503,\n",
       " -0.036340469666042194,\n",
       " -0.041968448976032864,\n",
       " -0.019456531736070159,\n",
       " -0.04384444207936309,\n",
       " -0.041968448976032864,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.0082005731160888079,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " -0.04384444207936309,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zcores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "newzscores = [x for x in zcores if abs(x) > 0.1]\n",
    "# the specific indices that I care about\n",
    "indes = []\n",
    "c = 0\n",
    "for x in zcores:\n",
    "    if abs(x) > 0.1:\n",
    "        indes.append(c)\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1234)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you select which features to remove? Why did that reduce overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info to answer question found in report\n",
    "X_new = data.iloc[:, indes].values\n",
    "X_train_new1, X_test_new1, y_train_new1, y_test_new1 = train_test_split(X_new, r, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loreer = LogisticRegression(C = 0.1)\n",
    "\n",
    "loreer.fit(X_train_new1, y_train_new1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.59642857142857142,\n",
       " 0.60357142857142854,\n",
       " 0.64642857142857146,\n",
       " 0.67500000000000004,\n",
       " 0.65000000000000002,\n",
       " 0.64642857142857146,\n",
       " 0.64642857142857146]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_z = []\n",
    "plot_c = []\n",
    "plot_d = []\n",
    "\n",
    "C = [.0001, .001, .01, .1, 1, 10, 100]\n",
    "for c in C:\n",
    "    plot_d.append(i)\n",
    "    plot_c.append(c)\n",
    "    q = LogisticRegression(C = c, max_iter = 100)\n",
    "    cf = q.fit(X_train_new1, y_train_new1)\n",
    "    plot_z.append(cf.score(X_test_new1, y_test_new1))\n",
    "index = [i for i in range(len(plot_z)) if plot_z[i] == max(plot_z)][0]\n",
    "plot_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78660714285714284"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loreer.score(X_train_new1, y_train_new1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decision Tree\n",
    "\n",
    "* Initialize your model as a decision tree with sklearn.\n",
    "* Fit the data and labels to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55714285714285716"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters\n",
    "* To test out which value is optimal for a particular parameter, you can either loop through various values or look into `sklearn.model_selection.GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "de = [None, 2, 5, 10]\n",
    "plot_d = []\n",
    "plot_score_train = []\n",
    "plot_score_test = []\n",
    "for d in de:\n",
    "    plot_d.append(d)\n",
    "    clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = d)\n",
    "    cf = clf.fit(X_train, y_train)\n",
    "    plot_score_train.append(cf.score(X_train, y_train))\n",
    "    plot_score_test.append(cf.score(X_test, y_test))\n",
    "index = [i for i in range(len(plot_z)) if plot_z[i] == max(plot_z)][0]\n",
    "\n",
    "tr = [0] + de[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 2, 5, 10]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.56428571428571428,\n",
       " 0.64642857142857146,\n",
       " 0.59285714285714286,\n",
       " 0.5714285714285714]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you choose which parameters to change and what value to give to them? Feel free to show a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8VXWd//HXmwPqUQks0eKiaBmlZlqMXbRywkJLwyxN\nZxovZUa/1OpRlDaVVs7kRE1lWkZlWvrLcRoiLEfKu3ZTDATJKEITDiiYHrx0TMTP/PH97uViey77\nwFlnw9nv5+OxH2ev7/rutT7rctZnre+6KSIwMzMDGNbsAMzMbMvhpGBmZgUnBTMzKzgpmJlZwUnB\nzMwKTgpmZlZo6aQg6SJJny51f0DSA5Iek/Q8SQdJ+lPuPqqZsQ4GSUslvW6g6zYwrO0k/UHSLrn7\nMknn9FB3uKSQNHEAxvs1Se/b3OH0Y3y3SjppsMbXSxzvkHRZs+PoL0nfkvQvzY6jCpJ2yNuZXZod\ny5BNCpLuldQl6VFJnZJ+JWm6pGKaI2J6RHw+1x8B/Cfw5ojYMSL+CnwOuCB3zxnk+E+SdGsv/Zfk\nlegxSRskPVHq/uSmjDMiJkXELQNdtwEfAK6NiDUDNLxGzQQ+LWn4II93I5K+U1p2T0paX+q+ajOG\ne5qka+rKBJwLnLe5cTfBF4Fzyv/DQ0VEPJ63M4P9P/AsQ27m1jkyIkYCu5P+CT4BfLeHursC2wFL\nSmW713U3rOoNTUTsk1eiHYFbgNNq3RHx74Mdz2Z6P/CDwR5pRKwE/gwcMdjjrovjlNKy/CJweWlZ\nHjnAozsEeCIi7urvDyUNy0mlKSLiz8Aa4M3NiqE3W/j/WMOGelIAICLWRcRc4F3AiZL2BZB0iaRz\nJb0YWJqrd0q6XtKfgT2Bq/Ie27aSRkn6rqTVkjryb9vysE6S9EtJX5H0V+CcXP4eSXdLeljSPEm7\n1+LKzSDTcxNVp6QLlbwUuAh4TR53Z3+nWdIpkm6WdL6kh4BPSdpL0g2SHpL0oKQfSBpV+s1KSYfk\n7+dK+mFuynlU0l2SXrGJdSdLWpj7XSHpv2vNQ5L2BMYD8+smYYyk6/JvbpA0oYfp3KhJJk/3jaXu\nvSVdm6f5D5LeUTeIG4G39jDsYZJ+JOn+vHxuzMum1v+yPH//N8f5a0l7lPofptTMtk7S14BN3qBK\neoOk23Icd0h6TanfdEl/yTH8WdLRkv4B+DJwaF6HVubqhwM31Q37HyUtyHH+WtIrS/3mSzpH0m3A\n34BdJf2/PF2P5nX3hFL9I/J8/nRex1ZKOq7U//n5/+ARpaP3L6p0NCNpvzyfH5b0e0n1SfFGel5e\n20iardQE3JnXn71K/XeUdIGkFaXlOSz3m5Ln77o8L99Vmv5y/MXRVx5e7X/4z8DCXD5LafvwiKTf\nSjqw9PsRkj4r6Z7c/zZJY0rDen6ut72kr+f5t1qpqXOb3G9snoedkv4q6efdzY9NFhFD8gPcCxza\nTfl9wAfy90uAc/P3iUAAw3saBvBj4FvADsAuwG3A+3O/k4CngNOB4UA7MA1YBrw0l30K+FVpeAH8\nFBgN7AasBQ4rDe/WBqf1RuCUurJTcjwfANpyPC8GpgDb5Ph/CXyp9JuVwCH5+7lAFzA1/35mOZ5G\n6wLb5rqnASOAY4D1wDm5/zTgzrrYLwPWAQfl318I3Jj7Dc/zbWLuvhU4qW66a3V3BDqAE/LvXgn8\nFZhUqn8scFsP83VYXg4jSUeRFwDz6+J8EJicp+2/gMtyv12Ax4C3534z8vI4qbtxlYZ5LnBJXdme\nOe435pjeRtpjHgWMAR4C9sx1xwEvyd9PA66pG9b/ktf/3P0C4FHgHXkenQI8AIzM/eeT1uG98nrT\nlpfZRFKSm5qXfW2cR+Tle2Ye3jF5WW6f+/8UuDjPz/3zuK7J/UYD9wPH5fG8Ok/bHqV4TwBu7mHe\nbQu8Oy/3dmAWG6+zl+bp3zXH9vo8DZNKy2p4Xnb7lab/uNIwinmaxxPA3Lws2ksx7pSX+9mk7cjw\n3O+zeZh75mX5yvzb2rCen+t9m7Q+jcrz5VrgX3O/r5MS/vC8TF4/oNvOgRzYlvSh56Twm9LMvYQG\nk0Jekf5eW/C57Hjghvz9JOC+bv4B31vqHkba29o9dwdwcKn/lcCZpeFtblJY3sfv3gncXuqu39Bf\nU+q3H/BYf+uSNmT18+U3PJMUTqyfTtLG9rJS9yjgadIGrD9J4Z9ry6fU/7u15Z+7Dwf+2OB83jmP\ne4dSnBeV+r8NuCt/fw8bb5CGAavZtKTweeCbdWW/JG3Inwc8DBwJbFtXp7uk8Gvg3aXuDwDX19VZ\nDLwzf58PfLyPmK+treekpPAQoFL/vwH7knamAhhX6vdVntnIvhf437phXw58tNT9dmBRg8trPLCB\ntOHcLn9/YTf1/g34QQ/DaCQpHNhLDG3AE7XxknZSpnRTr0gKOd71wK6l/m8CFufv/wlcQSlZDuSn\nJZqP6owjrbT9tTsp86/Oh22dpKOG8tUCK7r5zddK9R8i7ZmMK9W5v/T9b6SVY6BsFE8+dL+ydmhL\nSoo79/L7+th22IS6Y0kJpKe4Hibtidcr6kTEOtLe5thext+d3YGDavM/L4N3kZJLzUig2+Y5SW25\neWN5nl/Lcq/yPOtp+Y2tm4anefZ86M90nFQ3HfsDYyNdEHEi8BHgAUk/kfTCXoZVP7/HAn+pq/MX\nNl5H69ejt0u6XalJrpO0x12eJ2sib72y2nx5AfB0RHT0MOzdgTfWTec0Gl9eI5Sab+/Jy+suUjLe\nKU+ngHu6+ekE0rmlTVU/fz6Vm9fWkY7wtgF2VmpqfkED4xpP2vlZWpoPP+KZbc3nSa0KN0n6o6QP\nb0bsz9JSSSG3s44j7V321wrSkcLOETE6f54TEfuU6kQ3v3l/qf7oiGiPiF81ML76YW2K+mH8B2ka\nXhYRzyEdjVR94nA1G29gIP0T1iwCXpj/Ybqto3TeYxSwqpvhPw5sX+p+fun7CuC6uvm/Y0ScVqrz\nUuDOHmI/AXgL6WhnFPCiWkg91C9bXTcNw0j/7JtiBemIpDwdO0TE1wEiYm5EvJE0n1eRmrmg+3Vo\nEakZsWYVaWNcthtpj7amGI6k55CaNT4D7BIRo4GbaXyeDJNUTu7ldWEFcHU3y+tjpTq9La9TSCfS\nX5/X731rYZOmM4A9uvndCqCnRNrb+lVTnj+Hky6cmEZq9nke8CTpyGkDaR70lrTJsW4gHQ3X5sOo\niNgVICIejojTI2I30k7OOZJe1ccwG9YSSUHScyQdQTrkuiwiFvd3GBGxGvg58OU8vGGSXijpDb38\n7CLgLEn75DhGSTqmwVE+AIyvnVwaICNJK/k6pRO3H+uj/kC4FRiudA/IcKUTvcWJzIi4l3Se55V1\nvztS0mskbUtqUrklL4N6C4F3SGpXumDgPaV+c4F9JP1T3oscIelASZNKdd5AaubrzkhSEv0racPw\nb41ONKntfH9J05Qud/4Iqf1/U1wCHK90QnhYntZDJe0qaYKkt0hqJzVTPE5qaoO0Du2mja+KuZo0\nzTU/AQ6UdFRePieTNnw9nbxsJ+3FrgWelvR2oKH7VSLi8Tz+zyndm/Jy0vmDmtk5lnfmWLbJ68CL\nSnX6Wl5PAA9LGklab2rjfoLU3He+pF3yUeDrJQn4PnCUpLfl8l0kvSz/dCFwjNKFJnuTdhR6M5J0\n7uhB0hHCv5FaGGq+A3xB0sS8LF+h0sUepVgvJbUyPE/JbpIOBcjr1B459nWk5f00A2SoJ4WrJD1K\n2hP4V1Jb3MmbMbwTSAv696TD8B+x8aHtRiLix6S98ytKh7OHNziu60mXw94v6cHNiLnsbOBA0oo0\nF/ifARpujyLi76R24OmkeXYsacPw91K1bwH1NyVdRvqnfpB0jqKnf8YvkfbU1pBOYBY3ZeVmp6mk\nk4+rSU09XyCdkETSONIJ1J7uBfgeaa9tFWlZNHKEVxv3A6S9uJl5GnYDftvo7+uG9SfSCdtzSQnq\nXtIFDSJtoD9JSgAPkpqVzsg/vZq0x79W0l/ysG4GtqvtqETEKtJe7Tl52NOBt0bEI71M1ydIG+a/\nkq4Euqa7uj14H89cVPFN4IfkdSE3hU3Nde4nzffPkTeqSleqPb+X8c0iNS3dTzqauKmu/wdJOyB3\n5tjPIe3BLyWto58hraO3kY5IIF3K3k6at9+gtH71YA7pvM09wHLS/H+41P9cUsK9Kcf6DdI2pd7p\npHl0B+n/9WrSyWmAffLvHwVuAP49Im7vI66GaeOmP7PqSboD+GpE/CB3bwcsAN4Qg3jzjtJloksi\nYtZgjXNLIOlo4OiIePcWEMuFABHxwQbqfgv4ZUR8v/LAWpiTglVO6X6Gu0l7ZycC55OunGj63Zs2\nuHKzzAbS+nAQ8DPgHRFxbVMDs8KQuAPPtngvJZ2c3IF05cU7nBBa1k6k9vJdSU16ZzshbFl8pGBm\nZoWhfqLZzMz6YatrPtp5551j4sSJzQ7DzGyrcscddzwYEX1eFr3VJYWJEycyf379s9PMzKw3tcuS\n++LmIzMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJ\nwczMCpUlBUkXS1oj6a4e+kvS+ZKWSVok6RVVxVI2Z0EHB513PXuc+TMOOu965izo6PtHZmYtosoj\nhUuAw3rpfzjp/bh7AaeS3tdaqTkLOjhr9mI6OrsIoKOzi7NmL3ZiMDPLKksK+QXhD/VSZRrw/Uh+\nA4yW9IKq4gGYOW8pXes3bFTWtX4DM+ctrXK0ZmZbjWaeUxgHrCh1r8xlzyLpVEnzJc1fu3btJo9w\nVWdXv8rNzFrNVnGiOSJmRcTkiJg8Zkyf74jo0djR7f0qNzNrNc1MCh3AhFL3+FxWmRlTJ9E+om2j\nsvYRbcyYOqnK0ZqZbTWamRTmAifkq5BeDayLiNVVjvCoA8bxhaNfxrjR7QgYN7qdLxz9Mo46oNtW\nKzOzllPZ6zgl/RA4BNhZ0krgbGAEQERcBFwNvAVYBvwNOLmqWMqOOmCck4CZWQ8qSwoRcXwf/QP4\nYFXjNzOz/tsqTjSbmdngcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZm\nVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwU\nzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMr\nOCmYmVnBScHMzApOCmZmVnBSMDOzQqVJQdJhkpZKWibpzG767yTpx5IWSbpN0r5VxmNmZr2rLClI\nagMuBA4H9gaOl7R3XbVPAgsjYj/gBOBrVcVjZmZ9q/JI4UBgWUQsj4gngSuAaXV19gauB4iIPwAT\nJe1aYUxmZtaLKpPCOGBFqXtlLiu7EzgaQNKBwO7A+ApjMjOzXjT7RPN5wGhJC4HTgQXAhvpKkk6V\nNF/S/LVr1w52jGZmLWN4hcPuACaUusfnskJEPAKcDCBJwD3A8voBRcQsYBbA5MmTo6J4zcxaXpVH\nCrcDe0naQ9I2wHHA3HIFSaNzP4BTgJtzojAzsyao7EghIp6SdBowD2gDLo6IJZKm5/4XAS8FLpUU\nwBLgvVXFY2Zmfauy+YiIuBq4uq7sotL3XwMvrjIGMzNrXLNPNJuZ2RbEScHMzApOCmZmVnBSMDOz\ngpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRg\nZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7NCn0lB0jGSRubvn5I0W9Irqg/NzMwGWyNHCp+O\niEclHQwcCnwX+Ga1YZmZWTM0khQ25L9vBWZFxM+AbaoLyczMmqWRpNAh6VvAu4CrJW3b4O/MzGwr\n08jG/VhgHjA1IjqB5wIzKo3KzMyaos+kEBF/A9YAB+eip4A/VRmUmZk1RyNXH50NfAI4KxeNAC6r\nMigzM2uORpqP3g68DXgcICJWASOrDMrMzJqjkaTwZEQEEACSdqg2JDMza5ZGksKV+eqj0ZLeB1wL\nfLvasMzMrBmG91UhIr4k6U3AI8Ak4DMR8YvKIzMzs0HXZ1IAyEnAicDMbIjrMSlIujUiDpb0KPl8\nQq0XEBHxnMqjMzOzQdVjUoiIg/NfX2lkZtYiGrlP4dW1p6Tm7pGSXlVtWGZm1gyNXH30TeCxUvfj\nNPiUVEmHSVoqaZmkM7vpP0rSVZLulLRE0smNhW1mZlVoJCko36cAQEQ8TQMnqCW1ARcChwN7A8dL\n2ruu2geB30fEy4FDgC9L8hNYzcyapJGksFzSGZJG5M+HgOUN/O5AYFlELI+IJ4ErgGl1dQIYKUnA\njsBDpGcrmZlZEzSSFKYDrwU6gJXAq4BTG/jdOGBFqXtlLiu7AHgpsApYDHwoH4lsRNKpkuZLmr92\n7doGRm1mZpuikZvX1gDHVTT+qcBC4I3AC4FfSLolIh6pi2EWMAtg8uTJ8ayhmJnZgGjk3MB2wHuB\nfYDtauUR8Z4+ftoBTCh1j89lZScD5+VzFssk3QO8BLit79DNzGygNdJ89APg+aS9+ptIG/dHG/jd\n7cBekvbIJ4+PA+bW1bkPmAIgaVfSYzQaOV9hZmYVaCQpvCgiPg08HhGXkt7V3Od9ChHxFHAa6a1t\ndwNXRsQSSdMlTc/VPg+8VtJi4DrgExHx4KZMiJmZbb5Gnn20Pv/tlLQvcD+wSyMDj4irgavryi4q\nfV8FvLmxUM3MrGqNJIVZknYCPkVq/tkR+HSlUZmZWVP0mhQkDQMeiYiHgZuBPQclKjMza4pezynk\newY+PkixmJlZkzVyovlaSR+TNEHSc2ufyiMzM7NB18g5hXflvx8slQVuSjIzG3IauaN5j8EIxMzM\nnjFnQQcz5y1lVWcXY0e3M2PqJI46oP5JQQOvkTuaT+iuPCK+P/DhmJnZnAUdnDV7MV3rNwDQ0dnF\nWbMXA1SeGBo5p/APpc/rgHOAt1UYk5lZS5s5b2mREGq61m9g5ryllY+7keaj08vdkkaTHoNtZmYV\nWNXZ1a/ygdTIkUK9xwGfZzAzq8jY0e39Kh9Ijbyj+SpJc/Pnp8BS4MeVR2Zm1qJmTJ1E+4i2jcra\nR7QxY+qkysfdyCWpXyp9fwr4S0SsrCgeM7OWVzuZvEVefUR6vPXqiHgCQFK7pIkRcW+lkZmZtbCj\nDhg3KEmgXiPnFP4bKL8ic0MuMzOzIaaRpDA8Ip6sdeTv21QXkpmZNUsjSWGtpOK+BEnTAL8Ix8xs\nCGrknMJ04HJJF+TulUC3dzmbmdnWrZGb1/4MvFrSjrn7scqjMjOzpmjkPoV/lzQ6Ih6LiMck7STp\n3MEIzszMBlcj5xQOj4jOWkd+C9tbqgvJzMyapZGk0CZp21qHpHZg217qm5nZVqqRE82XA9dJ+h4g\n4CTg0iqDMjOz5mjkRPN/SLoTOJT0xrV5wO5VB2ZmZoOv0aekPkBKCMcAbwTuriwiMzNrmh6PFCS9\nGDg+fx4E/gtQRPzjIMVmZmaDrLfmoz8AtwBHRMQyAEkfGZSozMysKXprPjoaWA3cIOnbkqaQTjSb\nmdkQ1WNSiIg5EXEc8BLgBuDDwC6SvinpzYMVoJmZDZ4+TzRHxOMR8f8j4khgPLAA+ETlkZmZ2aDr\n1zuaI+LhiJgVEVOqCsjMzJqnX0nBzMyGNicFMzMrOCmYmVnBScHMzAqVJgVJh0laKmmZpDO76T9D\n0sL8uUvSBknPrTImMzPrWWVJQVIbcCFwOLA3cLykvct1ImJmROwfEfsDZwE3RcRDVcVkZma9q/JI\n4UBgWUQsj4gngSuAab3UPx74YYXxmJlZH6pMCuOAFaXulbnsWSRtDxwG/E8P/U+VNF/S/LVr1w54\noGZmlmwpJ5qPBH7ZU9NRvmFuckRMHjNmzCCHZmbWOqpMCh3AhFL3+FzWneNw05GZWdNVmRRuB/aS\ntIekbUgb/rn1lSSNAt4A/KTCWMzMrAGNvKN5k0TEU5JOI72+sw24OCKWSJqe+1+Uq74d+HlEPF5V\nLGZm1hhFRLNj6JfJkyfH/Pnzmx2GmdlWRdIdETG5r3pbyolmMzPbAjgpmJlZwUnBzMwKTgpmZlZw\nUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCpU9EM+slcxZ0MHMeUtZ1dnF2NHt\nzJg6iaMO6PadUmZbNCcFs800Z0EHZ81eTNf6DQB0dHZx1uzFAE4MttVx85HZZpo5b2mREGq61m9g\n5rylTYrIbNM5KQx1i66Er+wL54xOfxdd2eyIhpxVnV39KjfbkjkpDGWLroSrzoB1K4BIf686w4lh\ngI0d3d6vcrMtmZPCUHbd52B93d7q+q5UbgNmxtRJtI9o26isfUQbM6ZOalJEZpvOJ5qHsnUr+1du\nm6R2MtlXH9lQ4KQwlI0an5uOuim3AXXUAeOcBGxIcPPRUDblMzCirl17RHsqNzPrhpPCULbfsXDk\n+TBqAqD098jzU7mZWTfcfDTU7Xesk4CZNcxHCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMz\nKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMysUGlSkHSYpKWSlkk6s4c6h0haKGmJpJuq\njMfMzHpX2VNSJbUBFwJvAlYCt0uaGxG/L9UZDXwDOCwi7pO0S1XxtKo5Czr8RjAza1iVRwoHAssi\nYnlEPAlcAUyrq/NPwOyIuA8gItZUGE/LmbOgg7NmL6ajs4sAOjq7OGv2YuYs6Gh2aGa2haoyKYwD\nyu+CXJnLyl4M7CTpRkl3SDqhwnhazsx5S+lav2Gjsq71G5g5b2mTIjKzLV2zX7IzHHglMAVoB34t\n6TcR8cdyJUmnAqcC7LbbboMe5NZqVWdXv8ptMyy6Eq77HKxbmd6BPeUzfrmRbZWqPFLoACaUusfn\nsrKVwLyIeDwiHgRuBl5eP6CImBURkyNi8pgxYyoLeKgZO7q9X+W2iRZdCVedAetWAJH+XnVGKjfb\nylSZFG4H9pK0h6RtgOOAuXV1fgIcLGm4pO2BVwF3VxhTS5kxdRLtI9o2Kmsf0caMqZOaFNEQdd3n\nYH3d0df6rlRutpWprPkoIp6SdBowD2gDLo6IJZKm5/4XRcTdkq4BFgFPA9+JiLuqiqnV1K4y8tVH\nFVu3sn/lZlswRUSzY+iXyZMnx/z585sdhtkzvrJvbjqqM2oCfMT7OLZlkHRHREzuq57vaDbbXFM+\nAyPqztOMaE/lZlsZJwWzzbXfsXDk+enIAKW/R57vq49sq9TsS1LNhob9jnUSsCHBRwpmZlZwUjAz\n2xItujJdxHDO6PR3kO57cfORmdmWpnZDZO3+l9oNkVB5M6WPFMzMtjRNvCGy9ZJCkw7JzMwa1sQb\nIlsrKfgZNWa2NRg1vn/lA6i1koKfUWNmW4Mm3hDZWknBz6gxs61BE2+IbK2rj0aN7+EZNdUfkpmZ\n9UuTbohsrSMFP6PGzKxXrZUU/IwaM7NetVbzEfgZNWZmvWitIwUzM+uVk4KZmRWcFMzMrOCkYGZm\nBScFMzMrOCmYmVnBScHMzApOCmZmVlBENDuGfpG0FvjLAAxqZ+DBARjO1sLTO3S10rSCp3dT7R4R\nY/qqtNUlhYEiaX5ETG52HIPF0zt0tdK0gqe3am4+MjOzgpOCmZkVWjkpzGp2AIPM0zt0tdK0gqe3\nUi17TsHMzJ6tlY8UzMysjpOCmZkVWjIpSDpM0lJJyySd2ex4qiRpgqQbJP1e0hJJH2p2TFWT1CZp\ngaSfNjuWqkkaLelHkv4g6W5Jr2l2TFWS9JG8Ht8l6YeStmt2TANJ0sWS1ki6q1T2XEm/kPSn/Hen\nKmNouaQgqQ24EDgc2Bs4XtLezY2qUk8BH42IvYFXAx8c4tML8CHg7mYHMUi+BlwTES8BXs4Qnm5J\n44AzgMkRsS/QBhzX3KgG3CXAYXVlZwLXRcRewHW5uzItlxSAA4FlEbE8Ip4ErgCmNTmmykTE6oj4\nXf7+KGmjMa65UVVH0njgrcB3mh1L1SSNAl4PfBcgIp6MiM7mRlW54UC7pOHA9sCqJsczoCLiZuCh\nuuJpwKX5+6XAUVXG0IpJYRywotS9kiG8kSyTNBE4APhtcyOp1FeBjwNPNzuQQbAHsBb4Xm4u+46k\nHZodVFUiogP4EnAfsBpYFxE/b25Ug2LXiFidv98P7FrlyFoxKbQkSTsC/wN8OCIeaXY8VZB0BLAm\nIu5odiyDZDjwCuCbEXEA8DgVNy00U25Ln0ZKhmOBHSS9u7lRDa5I9xBUeh9BKyaFDmBCqXt8Lhuy\nJI0gJYTLI2J2s+Op0EHA2yTdS2oWfKOky5obUqVWAisjonbk9yNSkhiqDgXuiYi1EbEemA28tskx\nDYYHJL0AIP9dU+XIWjEp3A7sJWkPSduQTlTNbXJMlZEkUpvz3RHxn82Op0oRcVZEjI+IiaTlen1E\nDNk9yYi4H1ghaVIumgL8vokhVe0+4NWSts/r9RSG8In1krnAifn7icBPqhzZ8CoHviWKiKcknQbM\nI129cHFELGlyWFU6CPgXYLGkhbnskxFxdRNjsoFzOnB53sFZDpzc5HgqExG/lfQj4Hekq+oWMMQe\neSHph8AhwM6SVgJnA+cBV0p6L+m1AcdWGoMfc2FmZjWt2HxkZmY9cFIwM7OCk4KZmRWcFMzMrOCk\nYGZmBScFGzIkbZC0MD9F805JH5W0yeu4pE+Wvk8sP7lyE4Y1RtJv8+MoXlfX78OStt/UYZsNJCcF\nG0q6ImL/iNgHeBPpSbhnb8bwPtl3lYZNARZHxAERcUtdvw+THu72LPmpvmaDxknBhqSIWAOcCpym\npE3STEm3S1ok6f0Akg6RdLOkn+V3bFwkaZik80hP41wo6fI82DZJ385HIj+X1F4/3nxEcX0ex3WS\ndpO0P/BFYFoeXnup/hmk5/jcIOmGXPaYpC9LuhN4jaRXSrpJ0h2S5pUeefBCSdfk8lskvaTCWWqt\nIiL88WdjaHdEAAAB8ElEQVRIfIDHuinrJD1V8lTgU7lsW2A+6cFqhwBPAHuS7nD/BfDO+uEBE0l3\n0e6fu68E3t3N+K4CTszf3wPMyd9PAi7oIe57gZ1L3QEcm7+PAH4FjMnd7yLdhQ/p2fp75e+vIj3W\no+nLwZ+t+9Nyj7mwlvVmYD9J78zdo4C9gCeB2yJiORSPGTiY9HC5evdERO1RIXeQEkW91wBH5+8/\nIB0h9NcG0gMMASYB+wK/SI/7oQ1YnZ96+1rgv3M5pGRntlmcFGzIkrQnaQO7BhBwekTMq6tzCM9+\nFHFPz375e+n7BuBZzUcD5ImI2JC/C1gSERu9ZlPSc4DOiNi/ohisRfmcgg1JksYAF5GabIL0AMQP\n5MeII+nFpRfSHJifmjuM1Dxzay5fX6vfD7/imVdE/jNQf1K5O48CI3votxQYU3v3sqQRkvaJ9E6M\neyQdk8sl6eX9jNXsWZwUbCipnRheAlwL/Bz4bO73HdJjpX+XLy39Fs8cKd8OXEB6DPM9wI9z+Sxg\nUelEcyNOB06WtIj0dNoPNfCbWcA1tRPNZZFeGftO4D/yieeFPPMOgX8G3pvLlzCEXytrg8dPSbWW\nlpuPPhYRRzQ7FrMtgY8UzMys4CMFMzMr+EjBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys8H+E04ms\nWJfrWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbe1278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(tr, plot_score_train)\n",
    "plt.scatter(tr, plot_score_test)\n",
    "plt.title(\"Different Training(blue) and Test(orange) accuracies\")\n",
    "plt.xlabel(\"Depth of tree\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a single decision tree so prone to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Random Forest\n",
    "\n",
    "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(criterion = 'entropy', n_estimators=100)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)\n",
    "#am overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62142857142857144"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did you choose to change and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de = [None, 2, 5, 10, 20, 40]\n",
    "plot_d = []\n",
    "plot_score_train = []\n",
    "plot_score_test = []\n",
    "for d in de:\n",
    "    plot_d.append(d)\n",
    "    clf = RandomForestClassifier(criterion = 'entropy',n_estimators=100, max_depth = d)\n",
    "    cf = clf.fit(X_train, y_train)\n",
    "    plot_score_train.append(cf.score(X_train, y_train))\n",
    "    plot_score_test.append(cf.score(X_test, y_test))\n",
    "index = [i for i in range(len(plot_z)) if plot_z[i] == max(plot_z)][0]\n",
    "\n",
    "tr = [0] + de[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.61428571428571432,\n",
       " 0.61428571428571432,\n",
       " 0.625,\n",
       " 0.6071428571428571,\n",
       " 0.59999999999999998,\n",
       " 0.62142857142857144]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest classifier prevent overfitting better than a single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#each tree gets different data and is unidentical\n",
    "#more in writeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
